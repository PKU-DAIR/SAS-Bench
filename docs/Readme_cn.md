<p align="center">
    <img src="./assets/logo.svg" alt="Logo" />
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR/SAS-Bench">
            <img alt="Static Badge" src="https://img.shields.io/badge/SAS--Bench-black?logo=github">
        </a>
        <a href="https://github.com/PKU-DAIR/SAS-Bench">
            <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PKU-DAIR/SAS-Bench?logo=github&style=flat">
        </a>
    </p>
</p>

## SAS-Bench: A Comprehensive Benchmark for Short Answer Scoring with LLMs

[æ•°æ®é›†](https://huggingface.co/datasets/aleversn/SAS-Bench) | [è®ºæ–‡](https://arxiv.org/pdf/2505.07247) | [ä»£ç ](https://github.com/PKU-DAIR/SAS-Bench)

## ğŸ” é¡¹ç›®æ¦‚è¿°

SAS-Benchæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)çš„ç®€ç­”é¢˜è¯„åˆ†(SAS)åŸºå‡†æµ‹è¯•ã€‚åŸºäºä¸­å›½é«˜è€ƒçœŸå®è¯•é¢˜æ„å»ºï¼Œæœ¬åŸºå‡†æµ‹è¯•å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **1030é“è¯•é¢˜**è¦†ç›–9å¤§å­¦ç§‘é¢†åŸŸ
- **4109ä»½ä¸“å®¶æ ‡æ³¨çš„å­¦ç”Ÿç­”æ¡ˆ**
- **åˆ†æ­¥è¯„åˆ†**ä¸**åˆ†å¸ƒé”™å› åˆ†æ**
- **å¤šç»´åº¦è¯„ä¼°ä½“ç³»**ï¼ˆæ•´ä½“è¯„åˆ†ã€åˆ†æ­¥è¯„åˆ†ã€é”™å› è¯Šæ–­ä¸€è‡´æ€§ï¼‰

## ğŸš€ æ ¸å¿ƒç‰¹è‰²

### çªç ´ä¼ ç»ŸSASç³»ç»Ÿå±€é™
SAS-Benchè§£å†³äº†ä¼ ç»Ÿç®€ç­”é¢˜è¯„åˆ†ç³»ç»Ÿçš„å…³é”®ç¼ºé™·ï¼š

| ç»´åº¦           | ä¼ ç»ŸSASç³»ç»Ÿ   | SAS-Benchä¼˜åŠ¿      |
| -------------- | ------------- | ------------------ |
| **è¯„åˆ†ç²’åº¦**   | å•ä¸€æ€»åˆ†      | åˆ†æ­¥åˆ†è§£è¯„åˆ†       |
| **å¯è§£é‡Šæ€§**   | é»‘ç®±æœºåˆ¶      | å®Œå¤‡çš„é”™å› ç±»å‹ä½“ç³» |
| **ç­”æ¡ˆå¤šæ ·æ€§** | å•ä¸€å­¦ç§‘/é¢˜å‹ | è·¨å­¦ç§‘éæ¨¡æ¿åŒ–è¯„ä¼° |

### æ•°æ®é›†ç‰¹æ€§

<p align="center">
    <img src="./assets/annotation.png" alt="SASäººå·¥æ ‡æ³¨ç³»ç»Ÿ" width="50%" />
</p>

æ•°æ®é›†åŒ…å«ä¸‰ç±»é¢˜ç›®åŠä¸°å¯Œæ ‡æ³¨ï¼š

1. **é€‰æ‹©é¢˜**ï¼ˆè‡ªç”±å¡«å†™å½¢å¼ï¼‰
2. **å¡«ç©ºé¢˜**
3. **ç®€ç­”é¢˜**ï¼ˆå«æ­¥éª¤åˆ†è§£ï¼‰

æ¯ä»½ç­”æ¡ˆåŒ…å«ï¼š
- âœ… äººå·¥æ ‡æ³¨æ•´ä½“å¾—åˆ†
- ğŸ” æ­¥éª¤åˆ’åˆ†ä¸åˆ†é¡¹è¯„åˆ†
- âŒ æ­¥éª¤é”™å› å½’ç±»

## ğŸŒŸ è¯„ä¼°æ¡†æ¶

### CCSè¯„ä¼°ï¼ˆååŒä¸€è‡´æ€§è¯„åˆ†ï¼‰

**ç›®çš„**  
è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸äººå·¥è¯„åˆ†åœ¨æ•´ä½“å¾—åˆ†å’Œæ­¥éª¤å¾—åˆ†ä¸Šçš„ååŒä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹ç†è§£è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚

**å…¬å¼**  
è°ƒæ•´æƒé‡çŸ©é˜µç»“åˆæ•´ä½“ä¸æ­¥éª¤å·®å¼‚ï¼š
```math
W_{i,j} = \alpha \cdot \frac{(r_i - r_j)^2}{(N_r - 1)^2} + \frac{1 - \alpha}{m} \sum_{k=1}^{m} \frac{(s_{i,k} - s_{j,k})^2}{(N_{s_k} - 1)^2}
```
å…¶ä¸­ï¼š  
- $r_i, r_j$ï¼šæ¨¡å‹/äººå·¥æ•´ä½“è¯„åˆ†  
- $s_{i,k}, s_{j,k}$ï¼šç¬¬$k$æ­¥å¾—åˆ†  
- $\alpha=0.5$ï¼šå¹³è¡¡æƒé‡  
- $N_r, N_{s_k}$ï¼šå¯èƒ½å¾—åˆ†ç­‰çº§  

æœ€ç»ˆCCSè®¡ç®—ï¼š
```math
\text{CCS} := 1 - \frac{\sum_{i,j} O_{i,j} \cdot W_{i,j}}{\sum_{i,j} E_{i,j} \cdot W_{i,j}}
```

### ECSè¯„ä¼°ï¼ˆé”™å› ä¸€è‡´æ€§è¯„åˆ†ï¼‰

**ç›®çš„**  
é‡åŒ–æ¨¡å‹è¯†åˆ«é”™å› ç±»å‹çš„èƒ½åŠ›ï¼ŒæŒ‰ç­”æ¡ˆè´¨é‡åˆ†å±‚è¯„ä¼°ã€‚

**å…¬å¼**  
1. ä½¿ç”¨åˆ†ä½æ•°é˜ˆå€¼$\tau_1, \tau_2$å°†æ ·æœ¬åˆ†ä¸º3ç»„ï¼ˆä½/ä¸­/é«˜ï¼‰ï¼š
```math
\phi(x) = \mathbb{I}(x \geq \tau_1) + \mathbb{I}(x \geq \tau_2)
```
2. è®¡ç®—æ¯ç»„çš„é”™å› é¢‘ç‡çŸ©é˜µ$\mathbf{M}^p_k, \mathbf{M}^g_k$
3. è®¡ç®—ç»„å†…Spearmanç›¸å…³æ€§ï¼š
```math
\rho_k = \text{SpearmanR}(\mathbf{M}^p_k, \mathbf{M}^g_k)
```
æœ€ç»ˆECSï¼š
```math
\text{ECS} := \frac{1}{m} \sum_{k=0}^{2} \rho_k
```

**å…³é”®ç‰¹æ€§**  
- é‡‡ç”¨**3çº§æ€§èƒ½åˆ†å±‚**ï¼ˆm=3ï¼‰ç¡®ä¿ç¨³å¥è¯„ä¼°  
- å…³è”**é”™å› ç±»å‹åˆ†å¸ƒ**ï¼ˆè€Œéç®€å•è®¡æ•°ï¼‰  
- æ ‡å‡†åŒ–è¯„åˆ†æ”¯æŒè·¨æ•°æ®é›†æ¯”è¾ƒ

## âš™ï¸ å®‰è£…æŒ‡å—

### æ ¸å¿ƒä¾èµ–
```bash
pip install protobuf transformers>=4.44.1 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate json_repair openai
```

æˆ–ï¼š
```bash
pip install -r requirements.txt
```

### vLLMç¯å¢ƒé…ç½®ï¼ˆæ¨èï¼‰
```bash
conda create -n vllm python=3.12 -y
conda activate vllm
pip install vllm  # éœ€CUDA 12.0+
```

å…¶ä»–é…ç½®è¯·å‚è€ƒå®˜æ–¹[vLLMå®‰è£…æŒ‡å—](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html)ã€‚

## ğŸ“Š åŸºå‡†æµ‹è¯•æµç¨‹

![å·¥ä½œæµç¨‹](./assets/workflow.png)

### ç›®å½•ç»“æ„

```
|- discuss/        - åˆ†æè„šæœ¬
|- docs/           - æ–‡æ¡£èµ„æº
|- main/           - æ¨¡å‹è®­ç»ƒ/æ¨ç†ä»£ç 
|- prompts/        - é¢„å®šä¹‰æç¤ºæ¨¡æ¿
|- sas_pipelines/  - ä¸»è¦è¯„ä¼°ä»£ç 
|- utils/          - å·¥å…·å‡½æ•°
```

### å®æ–½é€‰é¡¹

#### 0. æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡æ³¨é˜¶æ®µï¼‰
- åŸå§‹æ ‡æ³¨æ•°æ®ä½äº`backend_data`
- è¿è¡Œ`preprocess.py`è¿›è¡Œæ•°æ®æ•´åˆ
- ä¿®æ”¹`DATANAME`å˜é‡æŒ‡å®šæºæ–‡ä»¶ï¼ˆä¸å«æ‰©å±•åï¼‰

> æ­¤æµç¨‹å¤„ç†æ¥è‡ªæˆ‘ä»¬æ ‡æ³¨ç³»ç»Ÿï¼ˆç³»ç»Ÿå³å°†å¼€æºï¼‰çš„åŸå§‹æ•°æ®

#### 1. æ•°æ®è·å–
æ•°æ®é›†å‘å¸ƒäº[HuggingFaceæ•°æ®é›†](https://huggingface.co/datasets/aleversn/SAS-Bench)ã€‚ä¸‹è½½æ–‡ä»¶å­˜æ”¾äº`datasets/`ï¼š
- æ–‡ä»¶å‘½åæ ¼å¼ä¸º`{q_id}_{course}_{question_type}.jsonl`
- é”™å› åˆ†ç±»ä½“ç³»åœ¨`error_type.jsonl`ä¸­ï¼š
  ```json
  {"q_id": 2, "course": "", "question_type": "", "guideline": "", "errors": [{"name": "", "description": ""}...]}
  ```
- `ID_Dict.json`åŒ…å«å­¦ç§‘-IDæ˜ å°„

#### 2. LLMé¢„æµ‹
æ”¯æŒJupyteræˆ–å‘½ä»¤è¡Œæ‰§è¡Œï¼š

**é€‰é¡¹Aï¼šJupyter Notebook**
- åœ¨`1_predict_scores.py`ä¸­è®¾ç½®`cmd_args = False`
- é…ç½®ï¼š
  - `save_type_name`ï¼šæ¨¡å‹æ ‡è¯†ç¬¦/è¾“å‡ºå‰ç¼€
  - `model_from_pretrained`ï¼šæ¨¡å‹è·¯å¾„
  - `file_name`ï¼šæ•°æ®é›†æ ‡è¯†ï¼ˆå¦‚`7_Math_ShortAns`ï¼‰

**é€‰é¡¹Bï¼šå‘½ä»¤è¡Œ**
è®¾ç½®`cmd_args = True`

*ä½¿ç”¨vLLMï¼ˆæ¨èï¼‰*ï¼š
```bash
cd sas_pipelines/
python 1_predict_scores.py --file_name=6_Chinese_ShortAns --save_type_name=<æ¨¡å‹ID> --model_from_pretrained=<è·¯å¾„> --batch_size=1000 --vllm=1
```

*å¯ç”¨Tensorå¹¶è¡Œ*ï¼š
```bash
python 1_predict_scores.py --n_gpu=0,1 --file_name=6_Chinese_ShortAns --save_type_name=<æ¨¡å‹ID> --model_from_pretrained=<è·¯å¾„> --batch_size=1000 --vllm=1 --tensor_parallel_size=2
```

*HuggingFaceé¢„æµ‹å™¨*ï¼š
```bash
python 1_predict_scores.py --file_name=6_Chinese_ShortAns --save_type_name=<æ¨¡å‹ID> --model_from_pretrained=<è·¯å¾„> --batch_size=5
```

*OpenAI APIé¢„æµ‹*ï¼š
1. åœ¨`sas_pipeline/`åˆ›å»º`api_key.txt`ï¼Œæ ¼å¼ï¼š
   ```text
   OpenAI <APIå¯†é’¥>
   Deepseek <APIå¯†é’¥>
   ```
2. æ‰§è¡Œï¼š
   ```bash
   python 1_predict_scores.py --file_name=6_Chinese_ShortAns --llm_name=deepseek-chat --save_type_name=Deepseek_V3
   ```

**é™„åŠ å‚æ•°**ï¼š
- ä½¿ç”¨å°æ ·æœ¬ç¤ºä¾‹ï¼š`--few_shot_num >0`
- ç¦ç”¨è¯„åˆ†æŒ‡å—ï¼š`--use_guideline=0`
- è·³è¿‡æ·±åº¦æ€è€ƒï¼š`--skip_thinking=1`
- `llm_name`é»˜è®¤ä¸º`save_type_name`ï¼ˆGLM3/OpenAIæ¨¡å‹é™¤å¤–ï¼‰

#### 3. é¢„æµ‹å¤„ç†
**é€‰é¡¹Aï¼šJupyter**
- åœ¨`2_process_prediction.py`è®¾ç½®`cmd_args = False`
- é…ç½®`file_name`ï¼ˆä½¿ç”¨`all`è¿›è¡Œæ‰¹é‡å¤„ç†ï¼‰

**é€‰é¡¹Bï¼šå‘½ä»¤è¡Œ**
```bash
python 2_process_prediction.py --file_name=all
```

#### 4. CCSè®¡ç®—
**é€‰é¡¹Aï¼šJupyter**
- åœ¨`3_compute_ccs.py`é…ç½®`file_name`å’Œ`save_type_name`

**é€‰é¡¹Bï¼šå‘½ä»¤è¡Œ**
```bash
python 3_compute_ccs.py --save_type_name=<æ¨¡å‹å‰ç¼€>
```

#### 5. ECSè®¡ç®—
**é€‰é¡¹Aï¼šJupyter**
- åœ¨`4_compute_ecs.py`è°ƒæ•´å‚æ•°

**é€‰é¡¹Bï¼šå‘½ä»¤è¡Œ**
```bash
python 4_compute_ecs.py --save_type_name=<æ¨¡å‹å‰ç¼€>
```

## ğŸ“ˆ Model Performance Insights

åœ¨16ä¸ªLLMsä¸Šè¿›è¡Œäº†å®éªŒ:

- QWK

![Workflow](./assets/qwk.png)

- CCS

| Models                 | Phy. (S.) | Phy. (M.) | His. (S.) | Geo. (S.) | Bio. (G.) | Chi. (G.) | Chi. (S.) | Math (S.) | Math (G.) | Pol. (S.) | Eng. (G.) | Che. (G.) | Avg.      |
| ---------------------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| Deepseek-R1            | 38.43     | **95.01** | **80.98** | 67.92     | **79.12** | 95.09     | 69.07     | 57.85     | **83.56** | 71.92     | 73.19     | 72.92     | 73.76     |
| QwQ-32B                | 48.53     | 87.23     | 75.43     | **77.06** | 72.52     | **96.00** | 31.77     | 48.66     | 45.51     | 74.48     | 54.79     | 62.17     | 64.51     |
| TinyR1-32B-Preview     | 38.17     | 84.88     | 75.83     | 71.52     | 73.45     | 92.57     | 52.61     | 48.28     | 74.77     | 70.70     | 57.92     | 41.37     | 65.17     |
| Qwen3-32B              | 47.29     | 85.51     | 64.96     | 80.43     | 63.15     | 92.21     | 50.43     | 51.26     | 80.77     | 73.30     | 59.33     | 57.82     | 67.20     |
| Qwen3-8B               | 54.33     | 76.17     | 45.54     | 68.89     | 43.22     | 86.01     | 42.02     | 46.33     | 73.33     | 64.25     | 50.55     | 50.52     | 58.43     |
| MiMo-7B-RL             | 52.77     | 41.01     | 61.33     | 67.10     | 35.93     | 54.72     | 43.09     | 38.09     | 55.79     | 36.78     | 34.69     | 31.05     | 46.03     |
| Deepseek-Prover-V2-7B  | 22.59     | 10.75     | 2.92      | 30.71     | 50.63     | 55.48     | 12.95     | 0.87      | 2.29      | 10.44     | 30.19     | 28.76     | 21.55     |
| DeepSeek-R1-Distill-7B | 33.71     | 29.24     | 50.92     | 32.35     | 52.18     | 52.44     | 44.29     | 29.52     | 39.55     | 53.77     | 32.98     | 34.27     | 40.44     |
| Deepseek-V3            | 53.89     | 85.72     | 69.85     | 76.23     | 76.51     | 93.42     | **69.49** | **58.81** | 80.18     | **76.75** | **73.82** | **74.64** | **74.11** |
| GPT 4o-mini-20240718   | **58.90** | 81.19     | 54.85     | 76.59     | 65.39     | 87.65     | 55.25     | 43.56     | 37.38     | 63.44     | 22.60     | 55.98     | 58.56     |
| Llama3.3-70B-Instruct  | 45.34     | 70.03     | 72.02     | 72.51     | 67.94     | 85.30     | 35.83     | 58.60     | 74.97     | 63.68     | 67.60     | 38.94     | 62.73     |
| Mixtral 8Ã—7B-Instruct  | 30.78     | 42.27     | 33.43     | 4.99      | 44.45     | 29.85     | 24.00     | 26.73     | 70.04     | 43.92     | 33.40     | 42.05     | 35.49     |
| Qwen2.5-32B-Instruct   | 40.53     | 77.02     | 62.34     | 74.50     | 72.07     | 94.85     | 66.37     | 50.08     | 32.59     | 64.09     | 53.35     | 62.87     | 62.56     |
| Qwen2.5-14B-Instruct   | 53.76     | 66.12     | 60.96     | 74.30     | 67.50     | 92.81     | 63.08     | 43.28     | 75.62     | 62.03     | 56.34     | 57.53     | 64.44     |
| GLM4-9B-Chat           | 45.62     | 52.33     | 36.81     | 69.41     | 39.19     | 63.92     | 42.94     | 35.50     | 56.95     | 54.83     | 33.92     | 30.79     | 46.85     |
| Llama3-8B-Instruct     | 41.09     | 35.10     | 37.52     | 31.29     | 32.19     | 38.13     | 32.89     | 23.55     | 62.43     | 37.78     | 31.68     | 29.27     | 36.08     |

- ECS

| Models                 | Phy. (S.) | Phy. (M.) | His. (S.) | Geo. (S.) | Bio. (G.) | Chi. (G.) | Chi. (S.) | Math (S.) | Math (G.) | Pol. (S.) | Eng. (G.) | Che. (G.) | Avg.      |
| ---------------------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- |
| Deepseek-R1            | 23.25     | 30.59     | 57.53     | 56.08     | 69.20     | 86.04     | 72.68     | **94.29** | 15.20     | 65.56     | _18.65_   | _81.76_   | **55.90** |
| QwQ-32B                | 4.74      | **63.92** | 67.06     | _70.04_   | 53.68     | 51.08     | 69.20     | 79.05     | 16.82     | 48.81     | -22.53    | 48.94     | 45.90     |
| TinyR1-32B-Preview     | 3.10      | **63.92** | 65.71     | **77.02** | 56.61     | 64.42     | 74.83     | 82.86     | 23.33     | 40.17     | -31.52    | 17.35     | 44.82     |
| Qwen3-32B              | -4.17     | 24.18     | _69.52_   | 54.29     | 53.67     | 52.70     | 47.31     | 82.21     | 18.33     | 62.14     | -26.99    | 36.27     | 39.12     |
| Qwen3-8B               | 23.39     | **63.92** | 14.29     | -4.96     | 52.21     | 47.75     | 34.01     | 39.20     | -8.14     | 57.19     | -27.13    | 59.28     | 29.25     |
| MiMo-7B-RL             | **51.05** | 24.18     | 14.29     | 38.85     | 58.35     | _92.17_   | 63.07     | 13.39     | 35.12     | -27.10    | -4.41     | 1.04      | 30.00     |
| Deepseek-Prover-V2-7B  | -24.10    | -5.20     | 42.86     | -6.23     | 29.54     | -80.81    | 23.25     | 46.67     | -1.51     | -58.64    | -45.23    | -21.91    | -8.44     |
| DeepSeek-R1-Distill-7B | -45.19    | 24.18     | 0.95      | -38.66    | 23.55     | -20.36    | 3.87      | -23.81    | -13.57    | -18.81    | -19.59    | -44.58    | -14.34    |
| Deepseek-V3            | 7.79      | 46.58     | 58.10     | 32.62     | _72.38_   | **96.58** | 57.43     | _92.38_   | _33.33_   | 40.26     | **24.77** | **85.83** | _54.00_   |
| GPT 4o-mini-20240718   | 17.91     | 24.18     | 62.14     | 36.68     | 55.20     | 79.01     | **78.00** | 67.62     | **46.90** | **92.31** | 10.04     | 36.39     | 50.53     |
| Llama3.3-70B-Instruct  | 22.56     | _57.35_   | 54.29     | 42.11     | 45.09     | 52.70     | 46.25     | 54.29     | 30.00     | 58.81     | -12.53    | -15.83    | 36.26     |
| Mixtral 8Ã—7B-Instruct  | 11.99     | 17.34     | **80.38** | 35.84     | 32.74     | 42.77     | 75.82     | 56.19     | 30.00     | 6.84      | -31.16    | -7.18     | 29.30     |
| Qwen2.5-32B-Instruct   | 11.95     | 17.41     | 53.33     | 59.34     | 62.96     | 46.90     | 75.08     | 62.86     | 30.00     | 46.67     | -4.50     | 27.08     | 40.76     |
| Qwen2.5-14B-Instruct   | 21.50     | 24.18     | 47.92     | 37.43     | **73.36** | 64.97     | 74.32     | 64.94     | 18.21     | 61.97     | -20.00    | 47.39     | 43.02     |
| GLM4-9B-Chat           | 35.00     | 24.18     | 32.49     | 34.73     | 62.12     | 20.36     | _77.34_   | 63.81     | **46.90** | _82.40_   | -25.35    | 7.18      | 38.43     |
| Llama3-8B-Instruct     | _48.25_   | 27.46     | 17.23     | 31.58     | 61.37     | -14.05    | 41.23     | 57.77     | 21.55     | -69.07    | -26.50    | -27.19    | 14.14     |

## ğŸ“… å¾…åŠäº‹é¡¹

- [ ] æä¾›è‹±æ–‡æœ¬åœ°åŒ–ç‰ˆæœ¬æ•°æ®é›†
- [ ] å¼€æºæ ‡æ³¨ç³»ç»Ÿï¼ˆå‰ç«¯ & åç«¯ï¼‰

## ğŸ“œ è®¸å¯å£°æ˜

SAS-Benché‡‡ç”¨`Apache License 2.0`åè®®å‘å¸ƒã€‚æœ¬æ•°æ®é›†ä»…é™ç ”ç©¶ç”¨é€”ä½¿ç”¨ã€‚

## ğŸ“š å¼•ç”¨æ–¹å¼

```bibtex
@article{lai2025sasbenchfinegrainedbenchmarkevaluating,
      title={SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models}, 
      author={Peichao Lai and Kexuan Zhang and Yi Lin and Linyihan Zhang and Feiyang Ye and Jinhao Yan and Yanwei Xu and Conghui He and Yilei Wang and Wentao Zhang and Bin Cui},
      year={2025},
      journal={arXiv preprint arXiv:2505.07247},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.07247}, 
}
```
